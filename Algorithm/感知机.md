# 感知机
$$f(x)=sign(\overrightarrow{w}·x+b)，sign(y)=
\begin{cases}
+1,y\geqslant 0\\\\
-1,y\leqslant 0
\end{cases}
$$

## 损失函数
$\because$

点到直线的距离为： 
$$\frac{1}{||w||}|\overrightarrow{w}·x_0+b|$$

且，对于误分类数据$(x_i,y_i)$来说
$$-y_i(\overrightarrow{w}·x_i+b)>0$$

$\therefore$

所以所有误分类点集合$M$ 到超平面的总距离为：
$$-\frac{1}{||w||}\sum_{x_i \in M}y_i(\overrightarrow{w}·x_i+b)$$

$\therefore$

感知机的损失函数（误差）为：
$$L(\overrightarrow{w},b)=-\sum_{x_i \in M}y_i(\overrightarrow{w}·x_i+b)$$

## 感知学习算法
最小化损失函数
$$\underset{(\overrightarrow{w},b)}{min}\ L(\overrightarrow{w},b)
=-\sum_{x_i \in M}y_i(\overrightarrow{w}·x_i+b)$$

## 梯度下降
随机选取一个误分类点$(x_i,y_i)$对$\overrightarrow{w},b$进行更新：
$$\overrightarrow{w}\leftarrow\overrightarrow{w}+\alpha x_iy_i\\\\
b\leftarrow b+\alpha y_i
$$

## 感知学习算法的对偶形式
从感知学习算法梯度下降中可以看出，
当$\overrightarrow{w}_0$值都为0时最终得到的$\overrightarrow w,b$可以表示为

$$
\overrightarrow{w}=\sum_{i=1}^N\alpha_i x_iy_i\\\\
b= \sum_{i=1}^N\alpha_i y_i
$$
$\therefore$
$$
f(x)=sign(\sum_{i=1}^N\alpha_i x_iy_i·x+b)
$$

# 参考
- 李航博士《统计学习方法》