# 信息量
一件事发生的概率越小，其发生后的信息量越大。
$$I(x)=-log_2P(x)$$

>我们只需要信息量满足低概率事件x对应于高的信息量。那么对数的选择是任意的。我们只是遵循信息论的普遍传统，使用2作为对数的底！

# 信息熵
信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。
$$H(X)=-\sum_{i\in X}P(x）log_2P(x)$$
当P(x)为连续型变量时，则信息熵的定义为：
$$H(X)=-\int_X P(x）log_2P(x)dx$$

>式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算

# 相对熵
相对熵(relative entropy)又称为KL散度（Kullback-Leibler divergence），KL距离，是两个随机分布间距离的度量。记为$D_{KL}(p||q)$。
$$
\begin{align}
D_{KL}(p||q) &= H_p(q)-H(p) \\\\
&=-\sum_{i\in X}P(x）log_2Q(x)+\sum_{i\in X}P(x）log_2P(x)\\\\
&=\sum_{i\in X}[P(x）log_2P(x)-P(x）log_2Q(x)]\\\\
&=\sum_{i\in X}P(x)log_2\frac{P(x)}{Q(x)}
\end{align}
$$
连续时表示为
$$D_{KL}(p||q) =\int_XP(x)log_2\frac{P(x)}{Q(x)}dx$$

# 交叉熵(Cross entropy)
相对熵的简化版 *(当$P(x)$固定时 )*
$$H_{CE}(p,q)=H(p)+D_{KL}(p||q)$$

可简化为
$$H_{CE}(p,q)=-\sum_{i\in X}P(x）log_2Q(x)$$
连续时为
$$H_{CE}(p,q)=-\int_XP(x）log_2Q(x)dx$$

# 参考
- https://www.zhihu.com/question/41252833/answer/195901726
- https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%86%B5/7302318?fr=aladdin
[^msgsum]:https://www.zhihu.com/question/22178202/answer/223017546